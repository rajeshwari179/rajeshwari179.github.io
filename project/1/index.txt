3:I[8173,["173","static/chunks/173-45ae19e0432c0727.js","118","static/chunks/app/project/1/page-8299d6d4471782c1.js"],"Image"]
6:I[9275,[],""]
7:I[1343,[],""]
8:I[3462,["231","static/chunks/231-af777def82264613.js","185","static/chunks/app/layout-7513d9a03965dc6e.js"],"default"]
2:T5be, The project focuses on accelerating the Scale-Invariant Feature Transform (SIFT) feature extraction process, a widely-used methodology for identifying interest points in images and videos. We aim to reduce the time required for this task by parallelizing various stages of the SIFT algorithm using CUDA-enabled GPUs. This approach substantially boosts the efficiency of video analysis, making it faster and more scalable compared to traditional methods. The motivation for our project arises from the growing demand for efficient video analysis across industries such as surveillance, entertainment, and healthcare. Our GPU-accelerated solution addresses this need by delivering significant efficiency gains, leading to cost savings and improved productivity for organizations that rely on video analysis for decision-making, monitoring, or research purposes. For our system, the expected input is a video file, and the output consists of the coordinates or descriptors of the interest points detected in each frame. Specifically, our system will generate a list of interest points along with their corresponding features for each frame. This output facilitates further analysis, object recognition, or tracking tasks, enabling researchers, analysts, and professionals to extract valuable insights from videos more efficiently. By improving the speed and accuracy of video analysis, we aim to enhance the overall efficiency and productivity of video analysis workflows.4:T8ff,Our primary metric for evaluating the success of our GPU-accelerated SIFT feature extraction approach is the time required to process each frame of the video. Specifically, we aim to achieve a significant reduction in processing time compared to traditional methods or serial implementations. Our key result demonstrates the successful parallelization of each stage of the SIFT (Scale-Invariant Feature Transform) algorithm, including processing up to the octaves, on our optimized GPU implementation. By adjusting the number of octaves and visualizing the impact of octave size, we optimized the SIFT algorithm's ability to detect features across various scales efficiently. This fine-tuning contributed to enhanced feature detection and robustness in diverse image datasets. We achieved a processing time of 0.00189 seconds for analyzing 64 images concurrently, with each image containing 5 layers and 3 octaves. The Difference of Gaussian (DoG) method is employed to effectively identify stable keypoint locations within the scale space of an image. This is achieved by convolving the image with the difference-of-Gaussian function, which is derived from the difference between two neighboring scales separated by a constant multiplicative factor k. One of the primary advantages of using this function is its computational efficiency. As the smoothed images L are already computed for scale space feature description, D can be easily obtained through straightforward image subtraction. Moreover, the DoG function closely approximates the scale-normalized Laplacian of Gaussian. In practice, the initial image undergoes incremental convolution with Gaussians to generate images at different scales, spaced by a constant factor k in scale space. For efficient octave processing, each octave is divided into s intervals, setting, k = 2^(1/s). To cover a complete octave during extrema detection, we produce (s+3) images in the blurred image stack for each octave. The DoG images, resulting from subtracting adjacent scales, are shown on the left. After processing a full octave, the Gaussian image with double the initial σ value is resampled by selecting every second pixel in each row and column. This resampling maintains accuracy relative to σ while significantly reducing computational overhead.5:T44c,We first attempted to run 1024 threads per block. However, some kernels, such as the row convolution kernel, were not able to run with this configuration. We then tried lowering the threads per block, which worked for all kernels. We also experimented with different block sizes and found that 16x48 (768 threads per block) was the optimal block size for row kernel. This is because the number of times apron elements are loaded into shared memory is minimized, and the number of threads that are actually used is maximized. It also allows for a half-warp memory coallescing, making it more efficient for older and while still being efficient for newer GPUs. Although the grid dimensions do not matter, we used a nearly square grid for all kernel to maximize the dimension of a grid since there is a hard limit on the number of blocks in each direction. We used the occupancy API to determine the optimal block size for each kernel. The occupancy API provides information about the maximum number of threads that can be run on a multiprocessor, helping us optimize the performance of our CUDA kernels0:["uVLmXGsQ9sfR-6H-USnM2",[[["",{"children":["project",{"children":["1",{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",{"children":["project",{"children":["1",{"children":["__PAGE__",{},[["$L1",["$","div",null,{"style":{"border":"2px solid #000","padding":"20px","borderRadius":"10px"},"children":[["$","h1",null,{"children":"A GPU-Accelerated Fast SIFT"}],[["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":"Introduction"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"$2"}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/sift.e77db125.png","height":920,"width":1480,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAeUlEQVR42mOoqW9IKay18ktlYLBnZmLg5Obm5OXmFuJnmNbeuKCvf/7EjllzQhgQQJjBy0Y71CG0pS1h6+4gQ317Ly/7gBAza0clBgZ5CQZRDgYQUA8O8g8KDGvp8N22z55B3FBOxkZZUk9OUl6Mmw8kn5IpX9mgDwA4WxwBK/L3OQAAAABJRU5ErkJggg==","blurWidth":8,"blurHeight":5},"alt":"Project Introduction","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 1: SIFTwith CUDA"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":"Capturing Videos for Dataset Creation"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"We curated our own dataset for the project, focusing on the Coda building. To maintain clarity and sharpness across the frames of the video, we used a gimbal during recording. This ensured minimal blurring and stable footage throughout, enhancing the quality of the video dataset. For accurate camera calibration, we use various checkerboard patterns. These patterns serve as a calibration tool due to their well-defined geometry and known dimensions. The known properties of the checkerboard patterns provides a ground truth reference helping in the precise estimation of camera parameters. To diversify our calibration dataset and avoid symmetrical patterns that could potentially skew the calibration results, we designed custom 3x3 grids."}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/dataset.b8c0cec8.jpeg","height":887,"width":1600,"blurDataURL":"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAoKCgoKCgsMDAsPEA4QDxYUExMUFiIYGhgaGCIzICUgICUgMy03LCksNy1RQDg4QFFeT0pPXnFlZXGPiI+7u/sBCgoKCgoKCwwMCw8QDhAPFhQTExQWIhgaGBoYIjMgJSAgJSAzLTcsKSw3LVFAODhAUV5PSk9ecWVlcY+Ij7u7+//CABEIAAQACAMBIgACEQEDEQH/xAAoAAEBAAAAAAAAAAAAAAAAAAAAAwEBAQAAAAAAAAAAAAAAAAAAAgP/2gAMAwEAAhADEAAAAJCx/8QAGxAAAgEFAAAAAAAAAAAAAAAAAQIDAAQTITL/2gAIAQEAAT8AmnluVfM7OW6JO6//xAAYEQEAAwEAAAAAAAAAAAAAAAABAAIDIf/aAAgBAgEBPwDEGnSf/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAIEIf/aAAgBAwEBPwCjHP/Z","blurWidth":8,"blurHeight":4},"alt":"Project Capturing Videos for Dataset Creation","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 2: Dataset"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":" Gaussian Filtering and Separability"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"In our project, we utilized a Gaussian filter for image blurring due to its separable nature, which offers computational advantages. Traditional 2D convolution filters require nxm multiplications for each pixel, where n and m are the dimensions of the kernel. In contrast, separable filters can be split into two one-dimensional filters applied sequentially along rows and columns. By taking advantage of this property, we divided the blurring process into horizontal and vertical stages. This method reduces the computational load to just n + m multiplications per pixel, making the Gaussian filter an efficient choice for image blurring while maintaining quality."}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/Gaussian.5aec0c9e.png","height":1827,"width":4421,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAATklEQVR4nGO885xN9P///wqMjLzXGBg4/jAwfGf+///9NwYGBkZGRqH/QAU83v//f0lnYY6sYGIU+AVkcwlwp998/cnGlJFR7CZQAX4TABvhKJy7qlpUAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":3},"alt":"Project  Gaussian Filtering and Separability","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure3: Gaussian Filter Separability"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":" Difference of Gaussian (DoG)"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"$4"}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/DoG.05fea62d.png","height":1952,"width":4420,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAECAYAAACzzX7wAAAAgUlEQVR42g3JsQrCMBRA0ftKk0IdXAStOAgKFXd/yW+sn+EgCOpWEHUoOqRRTF4zXLhw5Hov16quFtneVrPTBX6mbR+haQ4LEXln8J+AmQqmBBSsca4vEi5T4xxGQJdFPQIIoKpRvfcuzTePsXsVZn8uzO4DBPB9XW9CVc2f1lo/ALmzMuLSJH4oAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":4},"alt":"Project  Difference of Gaussian (DoG)","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 4: Difference of Gaussian (DoG)"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":"Large Parallelism"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"In order to make our code more efficient, we utilized large parallelism to process multiple images/frames simultaneously. This approach allows us to take advantage of the high memory bandwidth of modern GPUs, significantly reducing the time required for image processing. By processing 128 images at once, we were able to achieve optimal performance and enhance the speed of our SIFT algorithm. This also makes the code more scalable to handle hardware with higher memory bandwidths and processing capabilities in the future. Processing multiple images in parallel is a nice way to process a video because videos are essentially a series of images. This parallel processing capability is crucial for accelerating the feature extraction process and improving the efficiency of video analysis tasks.   "}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/LP.5c160d03.png","height":2221,"width":5684,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAVklEQVR42g3MPQ5AQAAF4XlsNpsQDYljOI27uYvL6CUaWyD+XvEV04y2Kzb3TSt4ypIe8bnXlMiSjpAzw3UyAYt1tluUGKtEDEXBZjOwWm2HBd/0wfsDfAMXtW7Jh2AAAAAASUVORK5CYII=","blurWidth":8,"blurHeight":3},"alt":"Project Large Parallelism","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 5: Large Parallelism"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":" Experiment Setup"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"$5"}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/gk.48d3d53c.png","height":601,"width":1862,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAYAAACuyE5IAAAAaUlEQVR4nA3FUQpFQBQA0HvnXdOYsplXys4sQPmS8ENZkJ3MhzChlLlD5vwcbNp6WJctjWN9BUJrDeFXKZUYY3psumJa5jsjijyzE1JKcM494chaW2JV54P3+D/33wEAAhHhCYgoYebxA5yAMEmw2zPEAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":3},"alt":"Project  Experiment Setup","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 6: Gaussian filter"}]]}]]}],["$","div",null,{"className":"ProjectTemplate_box__pk1Aa","children":[["$","div",null,{"style":{"flex":1},"children":["$","div",null,{"className":"ProjectTemplate_descriptionContainer__g40_B","children":[["$","h2",null,{"children":"Results"}],["$","p",null,{"className":"ProjectTemplate_projectDescription__RSofK","children":"Our primary metric for evaluating the success of our GPU-accelerated SIFT feature extraction approach is the time required to process each frame of the video. Specifically, we aim to achieve a significant reduction in processing time compared to traditional methods or serial implementations. Our key result demonstrates the successful parallelization of each stage of the SIFT (Scale-Invariant Feature Transform) algorithm, including processing up to the octaves, on our optimized GPU implementation. By adjusting the number of octaves and visualizing the impact of octave size, we optimized the SIFT algorithm's ability to detect features across various scales efficiently. This fine-tuning contributed to enhanced feature detection and robustness in diverse image datasets. We achieved a processing time of 0.00189 seconds for analyzing 64 images concurrently, with each image containing 5 layers and 3 octaves."}]]}]}],["$","div",null,{"className":"ProjectTemplate_imageSection__fIgKC","children":[["$","div",null,{"className":"ProjectTemplate_imageContainer__Xmpvw","children":["$","div",null,{"className":"ProjectTemplate_customImage__wSvfz","children":["$","$L3",null,{"src":{"src":"/rajeshwari179.github.io//_next/static/media/result.7dcc0435.png","height":368,"width":654,"blurDataURL":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAFCAIAAAD38zoCAAAAYElEQVR42mMorVQPChWPipWKiAGhqDiZoFCJkko1Br9AaQYUwAjE3n4SDBHRskCWjByHqDirmDibjCw7kBsWJQPUIcWACsA6xBmKylX9AkXCIiVCI8SBCMjwCxQtKlMBAK/GEtUHuj4ZAAAAAElFTkSuQmCC","blurWidth":8,"blurHeight":5},"alt":"Project Results","layout":"fill"}]}]}],["$","p",null,{"className":"ProjectTemplate_imageDescription__B6C1V","children":"Figure 7: DoG Results"}]]}]]}]]]}]],null],null]},["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","project","children","1","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/rajeshwari179.github.io/_next/static/css/3cc226f0877344cc.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","project","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en","children":["$","body",null,{"className":"__className_ae7d09","children":[["$","div",null,{"id":"title","className":"title"}],["$","$L8",null,{}],["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L7",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[],"styles":null}]]}]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/rajeshwari179.github.io/_next/static/css/e0b7e5d5ffdc86c6.css","precedence":"next","crossOrigin":"$undefined"}]],"$L9"]]]]
9:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Create Next App"}],["$","meta","3",{"name":"description","content":"Generated by create next app"}],["$","link","4",{"rel":"icon","href":"/favicon.ico","type":"image/x-icon","sizes":"16x16"}],["$","meta","5",{"name":"next-size-adjust"}]]
1:null
